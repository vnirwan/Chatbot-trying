{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vnirwan/Chatbot-trying/blob/main/fine_tune_VN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction\n",
        "This is a demonstration notebook that shows how to fine-tune a gpt-3 model. Using its \"ada\" model because it is the cheapest and fastest option available.\n",
        "\n",
        "The dataset used can be found at the following link: https://www.kaggle.com/datasets/aemreusta/paraphrased-articles-using-gpt3?resource=download.\n",
        "\n",
        "In order to keep costs low, only 10 data points were used for training and 3 data points were used for validation.\n",
        "\n",
        "The API key for this notebook is currently hardcoded in the notebook because it is unable to read it from a file or a variable. However, a solution to encrypt the key may be found later.\n",
        "\n",
        "One limitation to note is that this fine-tuning job was queued for approximately 2 hours and was interrupted multiple times.\n",
        "\n",
        "Dollar Used: 0.02"
      ],
      "metadata": {
        "id": "BKkK7EtTRNC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "import os\n",
        "import openai\n",
        "import getpass"
      ],
      "metadata": {
        "id": "L-IkJA6ZRE1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnoQSZN2uoYO",
        "outputId": "6332c5a6-fbd1-45e5-ac0e-2b267bff10e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/44.9 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.9/44.9 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m148.2/148.2 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BpRx7PZvEZX",
        "outputId": "3d8f0e7d-0132-42dd-da16-af8a947e70b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.8/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.8/dist-packages (from openai) (1.5.2.230105)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: types-pytz>=2022.1.1 in /usr/local/lib/python3.8/dist-packages (from pandas-stubs>=1.1.0.11->openai) (2022.7.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = getpass.getpass(prompt='Enter OpenAI API key:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0cddSu5vMV0",
        "outputId": "7a4e1e1a-a248-46c5-a9e5-63011c079066"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key:路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw=pd.read_csv('paraphrased_articles.csv')"
      ],
      "metadata": {
        "id": "yvSdkN2OvZo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take only two columns: abstract and ParaphrasedAbstract\n",
        "df_raw=df_raw[['Abstract','ParaphrasedAbstract']]\n",
        "#df.head()"
      ],
      "metadata": {
        "id": "nFtGmDYivsSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preparation\n",
        "Data preparation according to the gpt-3 models. Need to define \"prompt\" and \"completion\" and convert data to JSNOL format\n",
        "Rename *Abstract* to **prompt** and *ParaphrasedAbstract* to completion"
      ],
      "metadata": {
        "id": "DqvT9bdbEzd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.rename(columns = {'Abstract':'prompt', 'ParaphrasedAbstract':'completion'}, inplace = True)"
      ],
      "metadata": {
        "id": "0OJWDRNEwEb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Taking only first 10 rows to save money\n",
        "df=df_raw.head(10)\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe151C1lx-OG",
        "outputId": "67695a92-4ceb-49cc-c5b6-63156be255ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_json(\"paraphrasing_new.jsonl\", orient='records', lines=True)"
      ],
      "metadata": {
        "id": "dxZdqImNw_QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using data prepartion tool provided by openai to fix any issues with our dataset. -q will auto accept all suggestions."
      ],
      "metadata": {
        "id": "WGgQu9mjyl7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai tools fine_tunes.prepare_data -f paraphrasing_new.jsonl -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXPCT2GYxAhb",
        "outputId": "4ccf9c4f-a3cc-4dcd-a9b7-07d061330c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 10 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
            "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
            "- All completions end with suffix `.`\n",
            "  WARNING: Some of your completions contain the suffix `.` more than once. We suggest that you review your completions and add a unique ending\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified file to `paraphrasing_new_prepared.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"paraphrasing_new_prepared.jsonl\"\n",
            "\n",
            "After youve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 2.58 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing test dataset"
      ],
      "metadata": {
        "id": "oJtO6daOSg4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test=df_raw.tail(3)\n",
        "df_test.to_json(\"paraphrasing_new_test.jsonl\", orient='records', lines=True)\n",
        "!openai tools fine_tunes.prepare_data -f paraphrasing_new_test.jsonl -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6p1UuVU38aJ",
        "outputId": "2a7e5e3b-6d1f-4669-a85a-73e7cd8fc1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 3 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
            "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
            "- All prompts start with prefix `I`\n",
            "- All completions end with suffix `.`\n",
            "  WARNING: Some of your completions contain the suffix `.` more than once. We suggest that you review your completions and add a unique ending\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified file to `paraphrasing_new_test_prepared.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"paraphrasing_new_test_prepared.jsonl\"\n",
            "\n",
            "After youve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 2.48 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tool made some corrections in existing file and saved it with a new name.  "
      ],
      "metadata": {
        "id": "MrDIg1-YzFZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-Tuning\n",
        "Will use the new file created by data prepation tool to fine-tune ada model.\n",
        "Can use following command if api key is stored in a variable or a file(OPENAI_API_SECRET is the api key):\n",
        "!openai --api-key OPENAI_API_SECRET api fine_tunes.create -t \"paraphrasing_new_prepared\" -v \"paraphrasing_new_test_prepared\" -m ada"
      ],
      "metadata": {
        "id": "eT6EqpH9FHGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --api-key \"\" api fine_tunes.create -t \"paraphrasing_new_prepared.jsonl\" -v \"paraphrasing_new_test_prepared.jsonl\" -m ada"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1t7zaIV2nEz",
        "outputId": "16566570-e025-41e1-97b5-3e57be4ca84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rUpload progress:   0% 0.00/22.4k [00:00<?, ?it/s]\rUpload progress: 100% 22.4k/22.4k [00:00<00:00, 41.5Mit/s]\n",
            "Uploaded file from paraphrasing_new_prepared.jsonl: file-ghXpleojbafkD0E7TW8bVUn0\n",
            "Upload progress: 100% 4.34k/4.34k [00:00<00:00, 8.09Mit/s]\n",
            "Uploaded file from paraphrasing_new_test_prepared.jsonl: file-iWu5tGFiSjETc5aVwpI3wpiC\n",
            "Created fine-tune: ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-01-05 21:48:43] Created fine-tune: ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "[2023-01-05 21:49:50] Fine-tune costs $0.01\n",
            "[2023-01-05 21:49:50] Fine-tune enqueued. Queue number: 10\n",
            "[2023-01-05 21:50:39] Fine-tune is in the queue. Queue number: 9\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --api-key \"\" api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG8ni1DC2opI",
        "outputId": "607c39fe-27ff-4260-b0b6-1b56efb2b536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-01-05 21:48:43] Created fine-tune: ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "[2023-01-05 21:49:50] Fine-tune costs $0.01\n",
            "[2023-01-05 21:49:50] Fine-tune enqueued. Queue number: 10\n",
            "[2023-01-05 21:50:39] Fine-tune is in the queue. Queue number: 9\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --api-key \"\" api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YANDPPE7496X",
        "outputId": "c90916c5-fefc-496e-c4d6-2387afb1ae84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-01-05 21:48:43] Created fine-tune: ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "[2023-01-05 21:49:50] Fine-tune costs $0.01\n",
            "[2023-01-05 21:49:50] Fine-tune enqueued. Queue number: 10\n",
            "[2023-01-05 21:50:39] Fine-tune is in the queue. Queue number: 9\n",
            "[2023-01-05 22:15:42] Fine-tune is in the queue. Queue number: 8\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --api-key \"\" api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jDVx_3KDCsC",
        "outputId": "5ae081eb-ba13-46c3-d46c-caeb490c408c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-01-05 21:48:43] Created fine-tune: ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "[2023-01-05 21:49:50] Fine-tune costs $0.01\n",
            "[2023-01-05 21:49:50] Fine-tune enqueued. Queue number: 10\n",
            "[2023-01-05 21:50:39] Fine-tune is in the queue. Queue number: 9\n",
            "[2023-01-05 22:15:42] Fine-tune is in the queue. Queue number: 8\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --api-key \"\" api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agyXWYEEFbyL",
        "outputId": "c66c0db2-5dc5-4012-83c3-3779dc6c154f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-01-05 21:48:43] Created fine-tune: ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "[2023-01-05 21:49:50] Fine-tune costs $0.01\n",
            "[2023-01-05 21:49:50] Fine-tune enqueued. Queue number: 10\n",
            "[2023-01-05 21:50:39] Fine-tune is in the queue. Queue number: 9\n",
            "[2023-01-05 22:15:42] Fine-tune is in the queue. Queue number: 8\n",
            "[2023-01-05 22:38:08] Fine-tune is in the queue. Queue number: 7\n",
            "[2023-01-05 22:42:50] Fine-tune is in the queue. Queue number: 6\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --api-key \"\" api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aywegBiFGo5D",
        "outputId": "935ac795-72d4-4526-bbb8-ee151ab2cc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-01-05 21:48:43] Created fine-tune: ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "[2023-01-05 21:49:50] Fine-tune costs $0.01\n",
            "[2023-01-05 21:49:50] Fine-tune enqueued. Queue number: 10\n",
            "[2023-01-05 21:50:39] Fine-tune is in the queue. Queue number: 9\n",
            "[2023-01-05 22:15:42] Fine-tune is in the queue. Queue number: 8\n",
            "[2023-01-05 22:38:08] Fine-tune is in the queue. Queue number: 7\n",
            "[2023-01-05 22:42:50] Fine-tune is in the queue. Queue number: 6\n",
            "[2023-01-05 22:45:40] Fine-tune is in the queue. Queue number: 5\n",
            "[2023-01-05 22:49:08] Fine-tune is in the queue. Queue number: 4\n",
            "[2023-01-05 22:49:52] Fine-tune is in the queue. Queue number: 3\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --api-key \"\" api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNtqxjCEKrfA",
        "outputId": "37044019-5fa8-42e5-cb3e-bcd4c51c5376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-01-05 21:48:43] Created fine-tune: ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "[2023-01-05 21:49:50] Fine-tune costs $0.01\n",
            "[2023-01-05 21:49:50] Fine-tune enqueued. Queue number: 10\n",
            "[2023-01-05 21:50:39] Fine-tune is in the queue. Queue number: 9\n",
            "[2023-01-05 22:15:42] Fine-tune is in the queue. Queue number: 8\n",
            "[2023-01-05 22:38:08] Fine-tune is in the queue. Queue number: 7\n",
            "[2023-01-05 22:42:50] Fine-tune is in the queue. Queue number: 6\n",
            "[2023-01-05 22:45:40] Fine-tune is in the queue. Queue number: 5\n",
            "[2023-01-05 22:49:08] Fine-tune is in the queue. Queue number: 4\n",
            "[2023-01-05 22:49:52] Fine-tune is in the queue. Queue number: 3\n",
            "[2023-01-05 23:00:45] Fine-tune is in the queue. Queue number: 2\n",
            "[2023-01-05 23:01:30] Fine-tune is in the queue. Queue number: 1\n",
            "[2023-01-05 23:05:46] Fine-tune is in the queue. Queue number: 0\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --api-key \"\" api fine_tunes.follow -i ft-0gitoywCNU0H40CvBPvsvMJy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9L5DknaL2UI",
        "outputId": "f5108c6f-f596-49fc-e35f-0298e7de8985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-01-05 21:48:43] Created fine-tune: ft-0gitoywCNU0H40CvBPvsvMJy\n",
            "[2023-01-05 21:49:50] Fine-tune costs $0.01\n",
            "[2023-01-05 21:49:50] Fine-tune enqueued. Queue number: 10\n",
            "[2023-01-05 21:50:39] Fine-tune is in the queue. Queue number: 9\n",
            "[2023-01-05 22:15:42] Fine-tune is in the queue. Queue number: 8\n",
            "[2023-01-05 22:38:08] Fine-tune is in the queue. Queue number: 7\n",
            "[2023-01-05 22:42:50] Fine-tune is in the queue. Queue number: 6\n",
            "[2023-01-05 22:45:40] Fine-tune is in the queue. Queue number: 5\n",
            "[2023-01-05 22:49:08] Fine-tune is in the queue. Queue number: 4\n",
            "[2023-01-05 22:49:52] Fine-tune is in the queue. Queue number: 3\n",
            "[2023-01-05 23:00:45] Fine-tune is in the queue. Queue number: 2\n",
            "[2023-01-05 23:01:30] Fine-tune is in the queue. Queue number: 1\n",
            "[2023-01-05 23:05:46] Fine-tune is in the queue. Queue number: 0\n",
            "[2023-01-05 23:07:46] Fine-tune started\n",
            "[2023-01-05 23:08:03] Completed epoch 1/4\n",
            "[2023-01-05 23:08:04] Completed epoch 2/4\n",
            "[2023-01-05 23:08:06] Completed epoch 3/4\n",
            "[2023-01-05 23:08:07] Completed epoch 4/4\n",
            "[2023-01-05 23:08:28] Uploaded model: ada:ft-personal-2023-01-05-23-08-28\n",
            "[2023-01-05 23:08:29] Uploaded result file: file-0uWok98YXBK3TbxKgpJalWcz\n",
            "[2023-01-05 23:08:29] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded \n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal-2023-01-05-23-08-28 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuned Model: ada:ft-personal-2023-01-05-23-08-28"
      ],
      "metadata": {
        "id": "7VO6sP7NPNJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Results\n",
        "A result file is attached to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. Use fine tune job id to access this file."
      ],
      "metadata": {
        "id": "ggr0YP1lMhjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --api-key \"\" api fine_tunes.results -i ft-0gitoywCNU0H40CvBPvsvMJy > result.csv"
      ],
      "metadata": {
        "id": "MaOSi_O1Mw_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using this model on new data points\n",
        "Using 2 datapoints from our raw dataset. Adjust token size to determine number of words in the result(paraphrased sentence)."
      ],
      "metadata": {
        "id": "KB01XyvvNyix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new_data=df_raw.iloc[20:22,]\n",
        "df_new_data.to_json(\"new_datapoints.jsonl\", orient='records', lines=True)\n",
        "!openai tools fine_tunes.prepare_data -f new_datapoints.jsonl -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyR6PdcuNVHw",
        "outputId": "4aa7bef5-1f9d-49b3-8f6f-c4d5d7c848e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 2 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
            "- All prompts end with suffix `.`\n",
            "  WARNING: Some of your prompts contain the suffix `.` more than once. We strongly suggest that you review your prompts and add a unique suffix\n",
            "- All completions end with suffix `.`\n",
            "  WARNING: Some of your completions contain the suffix `.` more than once. We suggest that you review your completions and add a unique ending\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified file to `new_datapoints_prepared.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"new_datapoints_prepared.jsonl\"\n",
            "\n",
            "After youve fine-tuned a model, remember that your prompt has to end with the indicator string `.` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 2.47 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.read_json('new_datapoints_prepared.jsonl', lines=True)\n",
        "new_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "LFVrSarzPxZC",
        "outputId": "c7558ea9-85b4-44fa-e249-208b7655f5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              prompt  \\\n",
              "0  Pre-trained language models have been shown to...   \n",
              "1  Motion is an important cue for video predictio...   \n",
              "\n",
              "                                          completion  \n",
              "0   Pre-trained language models have been demonst...  \n",
              "1   Motion is an important factor for video predi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ceb73d3-0350-4fb4-8b7d-a5d0376fe46b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>completion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pre-trained language models have been shown to...</td>\n",
              "      <td>Pre-trained language models have been demonst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Motion is an important cue for video predictio...</td>\n",
              "      <td>Motion is an important factor for video predi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ceb73d3-0350-4fb4-8b7d-a5d0376fe46b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ceb73d3-0350-4fb4-8b7d-a5d0376fe46b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ceb73d3-0350-4fb4-8b7d-a5d0376fe46b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data['prompt'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "QZjs17J6TkZf",
        "outputId": "49dd4eb1-9920-4680-d29e-7dbb553bb5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the stateof-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model = 'ada:ft-personal-2023-01-05-23-08-28'\n",
        "new_res = openai.Completion.create(model=fine_tuned_model, prompt=new_data['prompt'][1] + '\\n\\n###\\n\\n', max_tokens=40, temperature=0)\n",
        "new_res['choices'][0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "RC3zyMFhPCkj",
        "outputId": "59bcfc6c-aa16-4f54-cef4-853120d8ab4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nThis paper is a stochastic model of motion in the video. It is based on the assumption that motion is deterministic. It is used to predict the future based on the motion history'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a sample text"
      ],
      "metadata": {
        "id": "g_jfx2VXQifK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_abstract = \"\"\"Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts are limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts from other high-quality ones can alleviate this limitation. When a single target contrast is of interest, common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods depending on their input.\"\"\""
      ],
      "metadata": {
        "id": "u8_e5ak1P9XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_res = openai.Completion.create(model=fine_tuned_model, prompt=sample_abstract + '\\n\\n###\\n\\n', max_tokens=20, temperature=0)\n",
        "new_res['choices'][0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7bALPyITQUY_",
        "outputId": "08f73302-3b35-48a6-a9c7-a4396c01722e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nThe present study aimed to evaluate the performance of multi-contrast MRI in the detection of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    }
  ]
}